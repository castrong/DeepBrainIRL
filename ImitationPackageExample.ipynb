{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a expert.\n",
      "Sampling expert transitions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 23:06:38.007 python[73813:4385294] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fdbc5834e90>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2023-04-23 23:06:38.008 python[73813:4385294] Warning: Expected min height of view: (<NSButton: 0x7fdbb15e7980>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2023-04-23 23:06:38.010 python[73813:4385294] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fdbb15e69b0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2023-04-23 23:06:38.011 python[73813:4385294] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fdbb13c0e70>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward before training: 9.0\n",
      "Training a policy using Behavior Cloning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 0         |\n",
      "|    ent_loss       | -0.000693 |\n",
      "|    entropy        | 0.693     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 72.5      |\n",
      "|    loss           | 0.693     |\n",
      "|    neglogp        | 0.694     |\n",
      "|    prob_true_act  | 0.5       |\n",
      "|    samples_so_far | 32        |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "209batch [00:00, 406.49batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward after training: 181.66666666666666\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"This is a simple example demonstrating how to clone the behavior of an expert.\n",
    "\n",
    "Refer to the jupyter notebooks for more detailed examples of how to use the algorithms.\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "\n",
    "def train_expert():\n",
    "    print(\"Training a expert.\")\n",
    "    expert = PPO(\n",
    "        policy=MlpPolicy,\n",
    "        env=env,\n",
    "        seed=0,\n",
    "        batch_size=64,\n",
    "        ent_coef=0.0,\n",
    "        learning_rate=0.0003,\n",
    "        n_epochs=10,\n",
    "        n_steps=64,\n",
    "    )\n",
    "    expert.learn(10000)  # Note: change this to 100000 to train a decent expert.\n",
    "    return expert\n",
    "\n",
    "\n",
    "def sample_expert_transitions():\n",
    "    expert = train_expert()\n",
    "\n",
    "    print(\"Sampling expert transitions.\")\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        DummyVecEnv([lambda: RolloutInfoWrapper(env)]),\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n",
    "        rng=rng,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts)\n",
    "\n",
    "\n",
    "transitions = sample_expert_transitions()\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "reward, _ = evaluate_policy(\n",
    "    bc_trainer.policy,  # type: ignore[arg-type]\n",
    "    env,\n",
    "    n_eval_episodes=3,\n",
    "    render=True,\n",
    ")\n",
    "print(f\"Reward before training: {reward}\")\n",
    "\n",
    "print(\"Training a policy using Behavior Cloning\")\n",
    "bc_trainer.train(n_epochs=1)\n",
    "\n",
    "reward, _ = evaluate_policy(\n",
    "    bc_trainer.policy,  # type: ignore[arg-type]\n",
    "    env,\n",
    "    n_eval_episodes=3,\n",
    "    render=True,\n",
    ")\n",
    "print(f\"Reward after training: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/castrong/opt/miniconda3/envs/HRIFinalProject/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/castrong/opt/miniconda3/envs/HRIFinalProject/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import seals\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms.adversarial.gail import GAIL\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "env = gym.make(\"seals/CartPole-v0\")\n",
    "expert = PPO(policy=MlpPolicy, env=env, n_steps=64)\n",
    "expert.learn(1000)\n",
    "\n",
    "rollouts = rollout.rollout(\n",
    "    expert,\n",
    "    make_vec_env(\n",
    "        \"seals/CartPole-v0\",\n",
    "        n_envs=5,\n",
    "        post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],\n",
    "        rng=rng,\n",
    "    ),\n",
    "    rollout.make_sample_until(min_timesteps=None, min_episodes=60),\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "venv = make_vec_env(\"seals/CartPole-v0\", n_envs=8, rng=rng)\n",
    "learner = PPO(env=venv, policy=MlpPolicy)\n",
    "reward_net = BasicRewardNet(\n",
    "    venv.observation_space,\n",
    "    venv.action_space,\n",
    "    normalize_input_layer=RunningNorm,\n",
    ")\n",
    "gail_trainer = GAIL(\n",
    "    demonstrations=rollouts,\n",
    "    demo_batch_size=1024,\n",
    "    gen_replay_buffer_capacity=2048,\n",
    "    n_disc_updates_per_round=4,\n",
    "    venv=venv,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net,\n",
    ")\n",
    "\n",
    "gail_trainer.train(20000)\n",
    "rewards, _ = evaluate_policy(learner, venv, 100, return_episode_rewards=True)\n",
    "print(\"Rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humanValg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
