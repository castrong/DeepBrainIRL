# Deep Brain IRL 

A repository for our experiments trying to find a way for (i) the fMRI data to inform the design of human driving models and/or (ii) human driving models to inform our analysis of the fMRI data. 

## Installation
1. Clone this repository
2. cd into the cloned repository (DeepBrainIRL), and install the conda environment using 
    ```conda env create -n DeepBrainIRL --file environment.yml ```
   then run ```conda activate DeepBrainIRL``` to activate the environment. 

3. cd into the ```deep_brain_irl``` directory and run ```pip install -e .``` (installs editable version of this package which we use for being able to import things from this package in each of our files in the package). 

## Data
We have four sub-directories in the dataset: ```BrainData```, ```Demonstrations```, ```LogAndLabelData```, and ```ROI Masks```, and ```Videos```. Files specific to a particular run will begin with yearmonthdaysubject_hour-minute-second_{type of file}.{extension}. The string yearmonthdaysubject_hour-minute-second will be referred to as the ID of a run. See ```src/utils/data_loading``` for examples of how to load each of these files. 


```BrainData```: this includes 3 types of files. Those ending in _combined data.npy are the raw files with shape(num TRs x num voxels matrices). Those ending in _filtered_removed_... .npy are the same format but have removed the predictions from the feature sets given in the filename (and so are residuals after fitting with these features). Those ending in _split.npy are the predictions from each feature space, with shape (feature space x TR x voxel). 

The indexing of the feature spaces is given by the following order ['Motion-Energy uncorrected', 'Motion-Energy recentered', 'Eyetracking', 'Depth', 'Affordance', 'Prompt Semantics Frame', 'Prompt Semantics Gaze', 'Head Direction', 'Controls', 'Turns Space Phase', 'Turns Time Phase', 'Future Path', 'Route Time Phase', 'Route Space Phase', 'Destination Vector Log', 'Destination-anchored Vector', 'Destination Grid Representation', 'Path Distance Remaining', 'Path Distance Elapsed', 'Beeline Distance Remaining', 'Beeline Distance Elapsed', 'Path Integration Egocentric', 'Path Integration Allocentric', 'Grid Cells', 'Road Graph', 'Vehicles Spatial', 'Pedestrians Spatial', 'Spatial Semantics', 'Route Time Binned', 'Route Space Binned', 'Gaze Grid', 'Gaze Direction', 'Scene Structure', 'Place Fields']

```Demonstrations```: These are files generated by ```parse_demonstrations.ipynb``` which describe trajectories for segments from the original line where the participant is driving straight (and not along the dirt road). An individual demonstration .npz file has a list of demonstrations. Each demonstration is a tuple with (state matrix, control matrix), with the dimension of the state matrix being (num time points x state dimension) and the dimension of the control matrix being (num time points x control dimension). The filenames include the number of closest other vehicles and pedestrians included in the state. The state is given by:
- Ego vehicle x, y, theta, speed (signed). 4 dimension.
- Closest other vehicles x, y, theta, speed (signed). 4 * num closest vehicles dimensions. 
- Closest pedestrians x, y, theta, speed (signed). 4 * num closest pedestrians dimensions.
- The centerline belonging to this straight segment, given by two points: x1, y1, x2, y2. 4 dimensions.
- The index within the training run this state is from. E.g. if the original run lasted 10 minutes at 15 fps you'd have 9,000 samples. This index could be anywhere from 0 to 8999, representing where in the run the particular state came from. 1 dimension.
- The index of the training run (given with respect to the order of IDs in the train, validation, and test ID files concatenated together in that order. 1 dimension. 

In total this gives 4 + 4*num closest vehicles + 4*num closest pedestrians + 4 + 1 + 1 dimensions. 

The control matrix is given by:
- throttle - brake
- steering wheel angle
- gear 
So in total 3 dimensions. 

Beyond raw demonstrations, this directory also includes the train IDs, validation IDs, and test IDs associated with the demonstration. These are each lists of strings stored as .npy files. 

```LogAndLabelData```: This subdirectory contains id_positions.xml files and id_scenario_labels.npz files. The id_positions.xml files are the files from the log of the runs used by Tianjiao's parsers in his driving-utilities Github. These contain the raw information about the state of the vehicle, other vehicles, and pedestrians. They are used by ```parse_demonstrations.ipynb``` to generate the data in the ```Demonstrations``` folder.

The id_scenario_labels.npz files contain two lists: (i) a list of timepoints associated with each type of driving scenario, under the key 'timestamps' (ii) a list of the scenarios under the key 'labels'. The scenario given at index i lasts from timepoint i to i + 1. The possible scenarios are 'straight', 'turning', 'transition', and 'dirt'. 

This directory also includes ```map_centerlines.npz```, which under the key 'lines' has a list of lines corresponding to the centerlines of the roads in the map (other than the dirt roads). These take the form of a 2-d list of the form [[x1, y1], [x2, y2]], e.g. [[1044-400, 61613], [9205, 61613]].

```ROI Masks```: 
These are num voxels long binary vectors for the participant SP which are True within the particular region of interest given in the filename and False elsewhere. 

```Videos```: These are videos of each run, with filename id_video.mp4. The first frame of the video aligns with the first frame of the raw data accessed from the id_positions.xml files. 


## Notebooks
The ```parse_demonstrations.ipynb``` notebook handles converting the raw state information from the logger and the labels (from ```Data/LogAndLabelData```) into sub-trajectories. 

The ```data_annotation.ipynb``` notebook contains the raw data labeling / annotations for the scenarios and the map. 

The ```bc.ipynb``` notebook gives an example of the data loading, preprocessing, and training required to run behavior cloning with a set of parameters and visualize the results. 

## Source Code Structure
We have two main subdirectories so far. The ```utils``` folder contains most of our code, with a variety of helper functions. ```data_loading.py``` has functions to load each of the types of data in ```Data/```, ```data_processing.py``` has code for processing the trajectories and brain data and combining them into training sets, ```delayer.py``` is a helper class taken from [the voxelwise modeling tutorials][https://github.com/gallantlab/voxelwise_tutorials/tree/main/voxelwise_tutorials] for stacking delayed versions of features, ```experiment_utils.py``` will have helper functions particular to experiments we setup, and ```visualization.py``` has a bunch of helper functions for visualizing and animating things. 

The ```learning``` subdirectory will include networks and training code, for now data preprocessing is the utils folder, but we can change that. 

## Experiments
The ```experiments``` folder is meant to include code to be able to reproduce experiments. The bulk of the code should be integrated into the ```src``` file, and the experiment makes appropriate calls to setup and run an experiment. 

